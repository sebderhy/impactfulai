{
  
    
        "post0": {
            "title": "Stable Diffusion Sampling Experiments",
            "content": "One of the great things with Diffusion Models (DMs) is that we can tune a bit their results without retraining them. Indeed, their inference process is somewhat an optimization process, and therefore can be tuned at several levels. . In this notebook, I try to analyze a bit the sampling process of Stable Diffusion, and derive from this analysis potential small variations (improvements?) of the images generated. . Notes: . I do not go into details of how diffusion models work here, here are some amazing references for those of you who want to learn more about them: | A great blog post from Yang Song (and the video from the same author) | fastai part2 2022 lesson 9 | The paper &quot;Elucidating the design space of diffusion models&quot; from Karras et. al., which gives an amazing unifying framework to reason about the different diffusion models | . Part of the text and code below are taken from this fastai notebook. Thanks to them for the amazing work done! . | I did not put a high emphasis on code quality here, because I wanted to mostly focus on the ideas and intuitions to understand Stable Diffusion. . | Imports And Utils . To run Stable Diffusion on your computer you have to accept the model license. It&#39;s an open CreativeML OpenRail-M license that claims no rights on the outputs you generate and prohibits you from deliberately producing illegal or harmful content. The model card provides more details. If you do accept the license, you need to be a registered user in 🤗 Hugging Face Hub and use an access token for the code to work. You have two options to provide your access token: . Use the huggingface-cli login command-line tool in your terminal and paste your token when prompted. It will be saved in a file in your computer. | Or use notebook_login() in a notebook, which does the same thing. | . Let&#39;s start by importing the libraries, select a GPU, and load a hf token . # !pip install -Uq diffusers transformers fastcore import torch torch.cuda.set_device(1) from PIL import Image from fastcore.all import concat import torch, logging from pathlib import Path from huggingface_hub import notebook_login from diffusers import StableDiffusionPipeline from PIL import Image import matplotlib.pyplot as plt logging.disable(logging.WARNING) if not (Path.home()/&#39;.huggingface&#39;/&#39;token&#39;).exists(): notebook_login() from tqdm.auto import tqdm . Very useful function to display a grid of images . def image_grid(imgs, rows, cols): w,h = imgs[0].size grid = Image.new(&#39;RGB&#39;, size=(cols*w, rows*h)) for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h)) return grid . Using Stable Diffusion . In order to always get the same results, we define a seed. . seed = 42 . Part of the prompts were taken from here. . prompts = [ &#39;A photo of an astronaut riding a horse&#39;, &#39;High-quality landscape photo of a futuristic city&#39;, &#39;Portrait photo of an asia old warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes, 50mm portrait photography, hard rim lighting photography&#39;, &#39;High-quality photo of beautiful beach landscape&#39; ] guidance_scale = 7.5 num_inference_steps = 50 device = torch.device(&quot;cuda&quot;) num_images_per_prompt = 1 . from diffusers import DiffusionPipeline, EulerDiscreteScheduler import torch repo_id = &quot;stabilityai/stable-diffusion-2-base&quot; # scheduler = EulerDiscreteScheduler.from_pretrained(repo_id, subfolder=&quot;scheduler&quot;) pipe = DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16, revision=&quot;fp16&quot;) pipe = pipe.to(device) . Fetching 12 files: 100%|██████████| 12/12 [00:00&lt;00:00, 21527.65it/s] . Let&#39;s start by running the pipeline &quot;off-the-shelf&quot;, without any modification . torch.manual_seed(seed) images = pipe(prompts, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps, num_images_per_prompt=num_images_per_prompt).images . 100%|██████████| 50/50 [00:56&lt;00:00, 1.13s/it] . len(images) . 4 . ref_images = images.copy() . image_grid(ref_images, rows=num_images_per_prompt, cols=len(prompts)) . Reproduce inference loop . In order to customize the inference loop, the first step is to reimplement it, and add some &quot;backdoors&quot;. More specifically, we will add 2 backdoors: . One that allows for a different update at each iteration | One that allows for post-processing after the sampling loop has been run | . These 2 customization points may look a bit random at this point, but we will see later how they can be useful . def regular_update(latents, i, t, text_embeddings, do_classifier_free_guidance): # expand the latents if we are doing classifier free guidance latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t) # predict the noise residual noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample # perform guidance if do_classifier_free_guidance: noise_pred_uncond, noise_pred_text = noise_pred.chunk(2) noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond) # compute the previous noisy sample x_t -&gt; x_t-1 latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample return latents . @torch.no_grad() def my_sd_sampling(prompt=prompts, device=device, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps, num_images_per_prompt=num_images_per_prompt, custom_update=None, post_processing=None, **kwargs): # 0. Default height and width to unet height = pipe.unet.config.sample_size * pipe.vae_scale_factor width = pipe.unet.config.sample_size * pipe.vae_scale_factor # 1. Check inputs. Raise error if not correct pipe.check_inputs(prompt, height, width, callback_steps=1) # 2. Define call parameters batch_size = 1 if isinstance(prompt, str) else len(prompt) # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2) # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1` # corresponds to doing no classifier free guidance. do_classifier_free_guidance = guidance_scale &gt; 1.0 # 3. Encode input prompt text_embeddings = pipe._encode_prompt(prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt=None) # 4. Prepare timesteps pipe.scheduler.set_timesteps(num_inference_steps, device=device) timesteps = pipe.scheduler.timesteps # 5. Prepare latent variables num_channels_latents = pipe.unet.in_channels latents = pipe.prepare_latents( batch_size * num_images_per_prompt, num_channels_latents, height, width, text_embeddings.dtype, device, generator=None, latents=None, ) num_warmup_steps = len(timesteps) - num_inference_steps * pipe.scheduler.order # 7. Denoising loop for i, t in enumerate(pipe.progress_bar(timesteps)): if custom_update: latents = custom_update(latents, i, t, text_embeddings, do_classifier_free_guidance) else: latents = regular_update(latents, i, t, text_embeddings, do_classifier_free_guidance) if post_processing: ## We are adding here the option of post-processing the final latent outputs latents = post_processing(**kwargs) # 8. Post-processing image = pipe.decode_latents(latents) # 9. Run safety checker image, has_nsfw_concept = pipe.run_safety_checker(image, device, text_embeddings.dtype) # 10. Convert to PIL images = pipe.numpy_to_pil(image) return images . torch.manual_seed(seed) imgs = my_sd_sampling() . 100%|██████████| 51/51 [00:56&lt;00:00, 1.11s/it] . Let&#39;s check that our pipeline gives the same results as the original one . import numpy as np assert np.sum(np.array(imgs[0])-np.array(ref_images[0]))==0 . . Log and analyze noise predictions . Below, we are going to log the 3 noise predictions that are performed at each timestep: . noise_pred_uncond (with just the noisy image as input) | noise_pred_text (with the noisy image + text encoding as input) | noise_pred (weighted average between them using the guidance_scale parameter) | . Log the noise predictions . def update_with_debug_logs(latents, i, t, text_embeddings, do_classifier_free_guidance): # expand the latents if we are doing classifier free guidance latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t) # predict the noise residual noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample # perform guidance if do_classifier_free_guidance: noise_pred_uncond, noise_pred_text = noise_pred.chunk(2) noise_unconds.append(noise_pred_uncond.detach().cpu()) ## Log the noise unconds results noise_conds.append(noise_pred_text.detach().cpu()) ## Log the noise conds results noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond) noise_preds.append(noise_pred.detach().cpu()) # compute the previous noisy sample x_t -&gt; x_t-1 new_latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample latent_step = new_latents-latents return new_latents . noise_preds, noise_unconds, noise_conds = [], [], [] torch.manual_seed(seed) imgs = my_sd_sampling(custom_update=update_with_debug_logs) . 100%|██████████| 51/51 [00:56&lt;00:00, 1.11s/it] . import numpy as np assert np.sum(np.array(imgs[0])-np.array(ref_images[0]))==0 . # + ref_images[len(prompts):] + imgs[len(prompts):], rows=2*num_images_per_prompt, cols=len(prompts)) . Compare the noise norms . One thing we observe from the code above (noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)) is that we actually pass the input image twice to the network at each iteration: . Once without the conditional text input -&gt; Predict the noise from the noisy image only | Once with the conditional text input -&gt; Predict the noise from the noisy image AND the image description&#39;s encoding | . Since these inputs are a bit different, we expect these 2 passes to predict different noise values (i.e. point towards different real images). However, since both networks are trained to predict noise from a noisy image, we should expect the noise prediction to have approximately the same norm in both cases. . Let&#39;s see if this is true, and let&#39;s also compare this norm with the final noise prediction norm. . noise_preds_norm = [pred.norm() for pred in noise_preds] noise_unconds_norm = [pred.norm() for pred in noise_unconds] noise_conds_norm = [pred.norm() for pred in noise_conds] . plt.scatter(range(num_inference_steps+1), noise_unconds_norm, c=&#39;b&#39;, alpha=0.5, label=&quot;Unconditional noise pred norm&quot;) plt.scatter(range(num_inference_steps+1), noise_conds_norm, c=&#39;g&#39;, alpha=0.5, label=&quot;Conditional noise pred norm&quot;) plt.scatter(range(num_inference_steps+1), noise_preds_norm, c=&#39;r&#39;, alpha=0.5, label=&quot;Final noise pred norm&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f681876a100&gt; . Here we indeed see that the unconditional and conditional noise norms are approximately the same. However, the final noise prediction seems to have a slightly different norm? Why is that? . The answer to this question is in the next line of code: . noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond) . This line does a weighted average between the conditioned and unconditioned noise prediction, and has therefore ABSOLUTELY NO REASON to have the same norm. . Does this make sense? Well... In my opinion, not so much, so let&#39;s try to re-normalize things :). . Improve the pipeline with &quot;whole&quot; rescale . NOTE: the idea implemented below was first suggested by Jeremy Howard in this Twitter thread . def update_with_rescale(latents, i, t, text_embeddings, do_classifier_free_guidance): # expand the latents if we are doing classifier free guidance latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t) # predict the noise residual noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample # perform guidance if do_classifier_free_guidance: noise_pred_uncond, noise_pred_text = noise_pred.chunk(2) noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond) noise_pred = noise_pred/noise_pred.norm()*noise_pred_uncond.norm() ### THIS IS THE KEY CHANGE # compute the previous noisy sample x_t -&gt; x_t-1 latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample return latents . torch.manual_seed(seed) imgs = my_sd_sampling(custom_update=update_with_rescale) . 100%|██████████| 51/51 [00:56&lt;00:00, 1.11s/it] . image_grid(ref_images + imgs, rows=2*num_images_per_prompt, cols=len(prompts)) . ref_images[0] . imgs[0] . ref_images[2] . imgs[2] . Definitely improves the image quality in my opinion! . Log and analyze latent space trajectory . Log trajectory . Below, we are going to log the trajectory point at each timestep, so that we can analyze a bit what&#39;s happening . def update_with_debug_logs(latents, i, t, text_embeddings, do_classifier_free_guidance): # expand the latents if we are doing classifier free guidance latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t) # predict the noise residual noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample # perform guidance if do_classifier_free_guidance: noise_pred_uncond, noise_pred_text = noise_pred.chunk(2) noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond) # compute the previous noisy sample x_t -&gt; x_t-1 new_latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample latent_step = new_latents-latents # THIS IS THE IMPORTANT LINES --&gt; Log information traj_pts_times.append(t.detach().cpu()) traj_pts_values.append(new_latents.detach().cpu()) return new_latents . traj_pts_times, traj_pts_values = [], [] torch.manual_seed(seed) imgs = my_sd_sampling(custom_update=update_with_debug_logs) . 100%|██████████| 51/51 [00:56&lt;00:00, 1.11s/it] . I find the fact that time is moving backward a bit unintuitive, especially for the kind of things we&#39;ll do next, so let&#39;s reverse it. . traj_pts_times[:10] . [tensor(981), tensor(961), tensor(961), tensor(941), tensor(921), tensor(901), tensor(881), tensor(861), tensor(841), tensor(821)] . traj_pts_times = [1000-t for t in traj_pts_times] . Visualize the trajectory using dimension reduction . Here we will perform 3 different types of dimension reduction in order to visualize the trajectory . T-SNE | PCA | MDS | . In these plot, yellow represent the enf of the sampling process, while the purple represents the beginning of the sampling process. . from sklearn.manifold import TSNE X = torch.stack(traj_pts_values).view(len(traj_pts_values), -1) X.shape . torch.Size([51, 65536]) . X_lowdim = TSNE(n_components=2, learning_rate=&#39;auto&#39;, init=&#39;random&#39;, perplexity=3).fit_transform(X) X_lowdim.shape . (51, 2) . plt.scatter(X_lowdim[:, 0], X_lowdim[:, 1], c=traj_pts_times) . &lt;matplotlib.collections.PathCollection at 0x7f67fe4a6af0&gt; . from sklearn.decomposition import PCA pca = PCA(n_components=2) X_lowdim = pca.fit_transform(X) X_lowdim.shape . (51, 2) . plt.scatter(X_lowdim[:, 0], X_lowdim[:, 1], c=traj_pts_times) . &lt;matplotlib.collections.PathCollection at 0x7f67fe422be0&gt; . I personally really like the MDS representation, because it tries to project the data in a way that best preserves distances. . from sklearn.manifold import MDS mds = MDS() X_lowdim = mds.fit_transform(X) X_lowdim.shape . (51, 2) . plt.scatter(X_lowdim[:, 0], X_lowdim[:, 1], c=traj_pts_times) . &lt;matplotlib.collections.PathCollection at 0x7f67fe422a90&gt; . We can see that the trajectory is quite smooth!! This is something we&#39;ll try to leverage later on. But first let&#39;s do some more analysis: . Analyze the steps length and directions . One interesting way to analyze the trajectory is to look at each step performed in the sampling loop: . How big was the step? | How much is it changing direction in the course of sampling? | . steps = [(traj_pts_values[i+1]-traj_pts_values[i]) for i in range(len(traj_pts_values)-1)] steps_norms = [step.norm() for step in steps] steps_unit_vecs = [step/step.norm() for step in steps] steps_dir_change = [(steps_unit_vecs[i+1]-steps_unit_vecs[i]).norm() for i in range(len(steps_unit_vecs)-1)] . fig, axs = plt.subplots(2, 1, figsize=(18, 12)) axs[0].scatter(range(len(steps_dir_change)), steps_dir_change) axs[0].set_title(&quot;Direction Changes Strength&quot;) axs[1].scatter(range(len(steps_norms)), steps_norms) axs[1].set_title(&quot;Step Length&quot;) . Text(0.5, 1.0, &#39;Step Length&#39;) . Interestingly, we see that the direction is changing a lot at the end. This tends to make me think that the algorithm overshoots somehow at the end. Let&#39;s try to fix this! . Prevent overshooting at the end . One very simple way to prevent this convergence overshooting at the end is to only do &quot;part of the way&quot;. That is, if the noise prediction says you should do a step size of x, you actually do a step of alpha*x, where alpha&lt;1. Let&#39;s try to implement something like this . def update_without_overshoot(latents, i, t, text_embeddings, do_classifier_free_guidance): # expand the latents if we are doing classifier free guidance latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t) # predict the noise residual noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample # perform guidance if do_classifier_free_guidance: noise_pred_uncond, noise_pred_text = noise_pred.chunk(2) noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond) # compute the previous noisy sample x_t -&gt; x_t-1 new_latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample ## THIS IS THE IMPORTANT CHANGE if i&gt;=round(0.8*num_inference_steps) and i!=num_inference_steps-1: ## When we get towards the end of the sampling new_latents = latents + 0.85 * (new_latents-latents) #do only 85% of the step supposed to be done. return new_latents . steps_unit_vecs = [] steps_magnitudes = [] . torch.manual_seed(seed) imgs = my_sd_sampling(custom_update=update_without_overshoot) . 100%|██████████| 51/51 [00:56&lt;00:00, 1.11s/it] . image_grid(ref_images + imgs, rows=2*num_images_per_prompt, cols=len(prompts)) . We can do a before/after comparison for one of the images . ref_images[-2] . imgs[-2] . The images look definitely sharper, but also a bit noisier unfortunately. . Trajectory predict . As we saw in the trajectory analysis, the sampling process seems to follow a relatively smooth curve, but the convergence kind of seems &quot;unfinished&quot;. So one natural idea would be to regress this trajectory with a polynomial, and then evaluate it at a later time in order to see what the results would have been after a few more iterations (which we don&#39;t want to do because we want to maintain the same computational budget). . def update_with_debug_logs(latents, i, t, text_embeddings, do_classifier_free_guidance): # expand the latents if we are doing classifier free guidance latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t) # predict the noise residual noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample # perform guidance if do_classifier_free_guidance: noise_pred_uncond, noise_pred_text = noise_pred.chunk(2) noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond) # compute the previous noisy sample x_t -&gt; x_t-1 new_latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample latent_step = new_latents-latents # Only change we do here in the update is logging the trajectory traj_pts_times.append(t.detach().cpu()) traj_pts_values.append(new_latents.detach().cpu()) return new_latents . This time, we are going to define and use the post-processing function . def compute_converged_latent(i_start=0, deg=5, eval_time=1000): bs = traj_pts_values[0].shape[0] pts = torch.stack([pt.flatten() for pt in traj_pts_values]) times = torch.Tensor([t for t in traj_pts_times]) times = 1000-times # We reverse time as previously coefficients, residuals, rank, singular_values, rcond = np.polyfit(times.float().detach().cpu().numpy()[i_start:], pts.float().detach().cpu().numpy()[i_start:], deg=deg, full=True) y_pred = np.polyval(coefficients, eval_time) converged_latents = torch.from_numpy(y_pred).view(bs, 4, 64, 64).half().to(device) return converged_latents . traj_pts_times, traj_pts_values = [], [] torch.manual_seed(seed) imgs = my_sd_sampling(custom_update=update_with_debug_logs, post_processing=compute_converged_latent, i_start=0, deg=6, eval_time=1024) . 100%|██████████| 51/51 [00:56&lt;00:00, 1.11s/it] . image_grid(ref_images + imgs, rows=2*num_images_per_prompt, cols=len(prompts)) . ref_images[1] . imgs[1] . ref_images[-1] . imgs[-1] . This seems to have denoising effect. Not sure if it&#39;s good or bad... Let&#39;s play a bit with the parameters . traj_pts_times, traj_pts_values = [], [] torch.manual_seed(seed) imgs = my_sd_sampling(custom_update=update_with_debug_logs, post_processing=compute_converged_latent, i_start=0, deg=5, eval_time=1200) . 100%|██████████| 51/51 [00:56&lt;00:00, 1.11s/it] . image_grid(ref_images + imgs, rows=2*num_images_per_prompt, cols=len(prompts)) . Ok, not exactly what we were expecting... This means that this polynomial fit technique is probably not very robust. But maybe it can be used as a new cool form of AI-generated art 😅. . Everything combined . def update_with_all_improv(latents, i, t, text_embeddings, do_classifier_free_guidance): # expand the latents if we are doing classifier free guidance latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t) # predict the noise residual noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample # perform guidance if do_classifier_free_guidance: noise_pred_uncond, noise_pred_text = noise_pred.chunk(2) dir_change = noise_pred_text/noise_pred_text.norm() - noise_pred_uncond/noise_pred_uncond.norm() noise_pred = noise_pred_uncond + guidance_scale * dir_change * noise_pred_uncond.norm() noise_pred = noise_pred/noise_pred.norm()*noise_pred_uncond.norm() ## Renorm step # compute the previous noisy sample x_t -&gt; x_t-1 new_latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample ## REDUCING SLIGHTLY THE OVERSHOOT if i&gt;=round(0.8*num_inference_steps) and i!=num_inference_steps-1: new_latents = latents + 0.9 * (new_latents-latents) latent_step = new_latents-latents traj_pts_times.append(t.detach().cpu()) traj_pts_values.append(new_latents.detach().cpu()) return new_latents . traj_pts_times, traj_pts_values = [], [] torch.manual_seed(seed) imgs = my_sd_sampling(custom_update=update_with_all_improv, post_processing=compute_converged_latent, i_start=0, deg=6, eval_time=1024) . 100%|██████████| 51/51 [00:56&lt;00:00, 1.11s/it] . image_grid(ref_images + imgs, rows=2*num_images_per_prompt, cols=len(prompts)) . ref_images[0] . imgs[0] . ref_images[1] . imgs[1] . ref_images[2] . imgs[2] . ref_images[3] . imgs[3] . Conclusion . In this notebook, we made 3 modifications: . rescale during the guidance weighted average | preventing the overshoot at the end | smoothing the sampling trajectory using polynomial fit | . Each of those modifications gave slightly different results taken independently. When combined together, they make the images MUCH sharper, which can be a good or bad thing, depending on the effect one wants to get at the end. . At the end of the day, I think the main takeaway is that understanding and controling the sampling process of diffusion models can have a very important impact, and understanding it can be crucial. .",
            "url": "https://sebderhy.github.io/impactfulai/2023/01/03/Stable-Diffusion-Sampling-Analysis.html",
            "relUrl": "/2023/01/03/Stable-Diffusion-Sampling-Analysis.html",
            "date": " • Jan 3, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "Applied Ai In A Data Centric World",
            "content": "How many times have we heard sentences such as “Data is the new oil” or “Data is food for AI”? Even though there is some truth to these catchphrases (after all, we data scientists/AI scientists do spend most of our time on the data), the vast majority of academic AI papers still revolve around new approaches that improve x% on famous dataset benchmarks such as ImageNet. Even in the industry, most data scientists often get their data as a given constraint for the problem they need to solve, and not as a variable they can optimize to better solve their task at hand. . . The idea to prioritize data over algorithms is not new. Actually, most leading AI companies (for example Tesla) are designing their AI workflows specifically to optimize data acquisition and tagging, However, this idea has been brilliantly reintroduced and explained by Andrew Ng in March 2021, and this is where (as far as I know) the term Data-centric AI (DCAI) was coined for the first time. Since Ng’s presentation, more and more people are joining forces to make AI more data-focused. Actually, a great hub was recently created to gather the most important resources and breakthroughs on this topic. . In this blog post, I do not want to explain the concepts behind DCAI in detail, since you can easily learn them from the links above. So before diving into the core ideas of this blog post, here is below the main idea of DCAI, so that you can continue reading even if you are not familiar with the field: . . . Even though the concept of DCAI is quite easy to grasp, I do think the implementation of DCAI in practice is far from obvious, so I want to discuss in this blog post the actual steps involved to make a successful DCAI project, and explain how it will change our job as AI/data scientists (non-related note: please forgive my lack of consistency in job titles, there’s unfortunately no standard across the industry). Basically, I want to answer the following questions: . Why now? What makes data-centric AI more relevant today? . | Aren’t data acquisition and model optimization two orthogonal tasks that can be performed in parallel by different people? . | Why and how does it completely change our work as Applied Scientists? . | What are the most important steps of a successful DCAI workflow? . | Where is the DCAI industry going in the future? . | . In particular, here are the main points I want to address in this post: . Iterating on the data is becoming much easier today, in particular thanks to the emergence of synthetic data (full disclosure: I work for a synthetic data provider called Datagen) and data labeling platforms. . | State-of-the-art models on the other hand are becoming a commodity. . | The DCAI methodology starts with engineering your TEST set (spoiler: I should have written test setS). . | In a DCAI world, the most important task AI scientists will have to perform is debugging their algorithm to find the optimal data their network needs (good news: it’s much more exciting than traditional debugging!). . | The future of DCAI will likely involve powerful AIs that would automatically debug neural networks, and derive from this debugging the kind of data that needs to be created. . | Why is data-centric AI happening now? . You may have noticed that the idea of DCAI is overall quite simple, obvious and even not so new. This fact induces 2 important questions: . Why would the DCAI revolution happen now? After all, deep learning started to take off almost 10 years ago now, and it has always been clear that data was the core component there. . | Aren’t data acquisition and model optimization two orthogonal tasks? In other words, maybe it makes sense that the AI scientists focus on the algorithm while someone else focuses on gathering more data? . | . The answer to the first question lies in the fact that up until now, data acquisition was a slow, complex, and expensive process, which involved a lot of operational challenges. In particular, there are 3 important operational (but not scientific!) challenges that need to be addressed when building a high-quality dataset: . Data diversity: you need to make sure your data is highly varied, has enough edge cases, but at the same time be careful to avoid biases in your dataset. For example, if all of your pictures come from a sunny country, your machine learning model may perform really badly on rainy days (side note: did you ever notice that most self-driving cars were all initially tested in sunny places?) . | Labeling: this step is a difficult and approximate science: different conventions lead to different labels. Sometimes, having the exact labels is even an impossible task (for example it is impossible for humans to extract the exact 3D information from an image, since an image is by definition 2D). . | Formatting: all data should be wrapped up in a format that contains all the information you need. . | . However, things have changed at many levels nowadays: . Creating state-of-the-art deep learning models has never been easier. Almost all the top AI papers now come out with open-source code, many python libraries such as huggingface, fast.ai, or pytorch lightning enable developers to train deep learning models with the latest architectures (for example, Transformers) and training tricks (e.g. one-cycle learning policy). . | More and more companies are selling data (some datasets can even be downloaded and used for free). However, while this solution is faster, it can still be very expensive, and most importantly will rarely cover your edge cases. . | The data labeling process has been considerably streamlined by products such as scale.com, Amazon Sagemaker, or Dataloop. Things like finding labelers to work on your data, creating labeling redundancy (to improve labeling consistency), and managing data are now much easier. . | The rise of synthetic data (computer-generated data) is a complete game-changer for the world of AI (in particular computer vision). This type of data enables companies to acquire realistic data with perfect labels and perfect control, at a fraction of the time (and cost) it takes to acquire real data. It has also been proven (in particular by Datagen and Microsoft) that synthetic data can significantly reduce the amount of real data you need to train a model. In practice, synthetic data gives a real superpower to AI scientists: being able to create the data they need. . | . Therefore, the same way the Internet made it so simple for entrepreneurs to create companies and iterate on their projects, the commoditization of data acquisition enables AI practitioners to quickly create a baseline for their model, and iterate efficiently on the data until it is ready to be deployed. . Let’s now address the second question mentioned at the beginning of the paragraph: . Is acquiring data part of an AI scientist’s job? . If you read the previous paragraph carefully, you probably already have the answer. A few years ago, acquiring data used to be an OPERATIONAL job, and had therefore no reason to be handled by an AI scientist (who would probably have done a very poor job anyway ^^). . However, the commoditization of data discussed above enables the AI scientist to fully engineer the data he or she uses to train a model. And as we will see, this engineering task is far from simple, and requires important research skills. . In other words, in most cases, I would say that YES, part of our job as AI scientists is to gather the data you need to train a model. . The 5 steps of a data-centric AI development . Ok, all this sounds very cool, but what does it mean in practice? If my job is not so much to train models anymore, what should I do then? How does this DCAI methodology get implemented in practice and what is our role in it as scientists? . (Rest assured, you will see that your scientific skills are critical to the success of a DCAI project!) . Step #1: Carefully engineer your test setS!! . This is something that few people talk about, but the implementation of data-centric AI should actually start with your TEST SET (or more precisely validation set as we’ll discuss later). . In principle, the test set (and metrics achieved on this test set) is the ground for many very impactful business decisions. In particular, the results of your algorithm on your test set will likely be the key element to decide whether or not it should be deployed to production. However, very often, data scientists create their test sets just by taking a random split of their training set. While this may be fine for a PoC, it will quickly show its limit if you want to build a robust algorithm that won’t fail every 2 days in production. . The first 2 things a data-centric AI scientist needs to build are: . A generic test set, which represents as closely as possible the probability distribution of cases you expect to get in production. . | Several “unit test sets”, which consist in designing specific test sets that are meant to measure whether your algorithm is robust to specific cases (for example: can a car detect pedestrians in low-light conditions). This approach was in particular well explained by Tesla’s Head of AI Andrej Karpathy. . | . Source: slide from Andrej Karpathy’s (Tesla’s Director of AI) presentation at Tesla 2019 Autonomy Day . Step #2: Get your end-to-end data-centric pipeline running . Finish your test environment . Although building the test set is generally the most laborious step to building your evaluation environment, you cannot achieve much if you just have data. Ideally, your evaluation process should also include: . Proper metrics that describe the trade-offs you’ll need to make in production (for example, execution time vs accuracy). And yes, there ARE ALWAYS trade-offs. . | Debug visualizations that show things that your metrics cannot quantify, if possible in a way that allows you to understand the problem more specifically. For example, if you build an image denoising algorithm, you should have a visualization of the inputs/outputs/ground truths side-by-side so that you can compare them and detect artifacts such as blur. . | A one-push-button system (e.g. python script) to evaluate any given method according to all of the points described above. . | As a side note, throughout my career, I’ve been struggling to find good tools to help me build test environments more quickly. While many MLOps solutions exist today (Weights &amp; Biases, Tensorboard, etc…), they are usually very focused around Machine Learning (which imposes a constraint on the solution to your problem), and are often more about debugging your networks than evaluating a given solution. Today, I am more and more using a tool called QA-Board, an open-source software released by Samsung which provides a complete environment for a combined quantitative and qualitative evaluation environment. ~~ ~~ . Build your baseline . In order to finish your end-to-end data-centric pipeline, you also need a baseline, i.e. the quickest solution you can find to your problem that can be evaluated in your environment. I talked about this more in depth in my first post “Fall in love with the problem”, but it’s worth saying it again: your baseline SHOULD NOT take you a long time to build, and SHOULD NOT give good performances. The goal is to check that all the blocks of your pipeline are integrated, and to be able to “appreciate” the quality of the results you’ll get when you will use more complex solutions. Very often, you actually don’t even need a training set to build a baseline: you can either go for non-ML solutions, or leverage the ocean of open-source code available online. . Ok, now that you have your end-to-end pipeline running, it is time to improve on your baseline, and this is where things will spice up from an algorithmic perspective. . Step #3: Discover the data you really need . Thanks to the efforts you put in step 1 and 2, you may be able to get some initial insights on your baseline’s performances. For example, it may fail on specific unit tests, and this may give you a hint regarding the type of data you want to gather. . But more often than not, defining accurately the data you need is much harder than it looks, because the failure modes of your algorithm are in general unknown in advance, and your “unit test sets” are only wild guesses of the kind of data that could cause an algorithm to fail. . Surprisingly, I did not find a lot of academic papers answering the question “What are the 2-3 main characteristics in my data that cause my network to fail”. As far as I can see, there are 3 potential approaches there: . Deploy the algorithm in shadow mode . . Even though your baseline is far from robust enough, it may be a good idea to put it in production in “shadow mode”, meaning that its predictions will not affect anything, but you’ll be able to gather a lot of data and monitor the kind of corner cases you’ll have to face. Here is a quick table of pros &amp; cons for this method: . Pros It forces the company to perform the integration work, which as discussed previously is one of the biggest failure risks of an AI system. . | This gets you exposure to the EXACT production distribution, with all its corner cases and so on. You can therefore extract all the failure modes, and add them directly to your training/test sets until you are satisfied with the algorithm’s performances. . | You can (theoretically) access all the production logs and context to understand precisely the context in which failure occurs. In contrast, when you work with “offline” data, you can only access the context which you have proactively collected at the dataset collection time. . | . . Cons | Detecting when neural networks fail is a hard open research problem. In a classification task, you can use the network’s confidence, but even then, it is known to be quite unreliable. . | Integrating a network to production (especially in shadow mode) has a strong cost for your company, and the company may rightfully decide to wait until you’ve shown promising results to make this investment. . | It requires the company to decide upfront HOW your algorithm will be integrated in production. This reduces significantly your algorithmic freedom afterwards, and may limit the performance achievable eventually. . | . | . Data attributes inference . Whenever your test set is big (and it should be), it is often very hard to find interpretable characteristics of your data that correlate with your network’s failures. But if you enrich your unit test sets with metadata (such as number of people, location of objects, etc…), you’ll be able to leverage many classical data science techniques (KDE, clustering) across INTERPRETABLE dimensions of your data. For example, if you run an inference network on each of your test images to infer the gender of the subject for each image, you may suddenly realize that your algorithm performs much better on men than women, which is a directly actionable insight from a data perspective since you can add more women to fix this issue. In order to perform this enrichment, you can take advantage of all the off-the-shelf networks that exist today. For example, Google’s Mediapipe project makes it very easy for anyone to extract information (3D face landmarks, body pose, etc…) about images, with relatively high robustness and accuracy. . Another possibility is to use synthetic data as a test set. Since synthetic data are built by an algorithm, every metadata you wish to have about this data is theoretically accessible. For more details about this approach, I highly recommend reading Datagen’s Neurips paper. . Unstructured failure modes discovery . Unfortunately, in many cases, you won’t know beforehand the characteristics of your data that are causing a network to fail. Ideally, we would like to have a toolbox that analyzes the model’s performances, and gives us actionable insights on how to improve them. . While the academic literature is relatively scarce on this topic, here are 2 interesting works I came across recently: . Stylex: very recent and interesting paper by Google that shows how to automatically discover and visualize the main data characteristics that affect a classifier. In other words, their method will help you identify and visualize exactly the attributes that cause a dog vs cat classifier to classify an image as dog or cat. . | . Explaining a Cat vs. Dog Classifier: StylEx provides the top-K discovered disentangled attributes which explain the classification. Moving each knob manipulates only the corresponding attribute in the image, keeping other attributes of the subject fixed. Source: Google’s blog post . Virtual Outlier Synthesis: another amazing and recent paper. They basically found a way to generate images that are out-of-distribution, i.e. corner cases that the network hasn’t seen a lot during training, and is therefore struggling on. Although this paper doesn’t really bring an explanation as to WHY these outliers are failing, the fact that it can generate new failure case candidates by itself is pretty amazing. . | . As part of my work at Datagen, I’ve also developed such a method to detect neural networks failure modes in an unstructured way. More precisely, I built a Variational Auto-Encoder that can transform an input failure case into the most similar image that would make the network succeed. When trained and tested on the facial landmarks detection task (on the 300W dataset), we observed for example that the VAE was removing glasses from people, and transformed babies into grown-up faces, hinting that the network has more difficulties dealing with babies and people with glasses. . . (As you may know, one of the famous problems with VAEs is that they create blurry pictures, as can be seen above. While this may be an issue in many cases, it is not in our “debugging” case, because these images are meant to be interpretable, not beautiful). . Step #4: Gather the new data and adapt your training pipeline . Once you’ve identified the type of data you need, you need to gather more of this data. This is typically where things may get harder: if you need real data, you may need to write a long list of precise requirements, and wait for 6 months until you get the new dataset back. This is exactly why I believe synthetic data is going to be a revolution. You can now use platforms like Datagen to create data with the exact characteristics of your failure cases, and a lot of variance on everything else. . However, keep in mind that your training procedure may need to be adapted if you have a new dataset. In particular, if you use synthetic data, both our team and Microsoft observed that 2 important changes need to happen in your training procedure: . More data augmentation is needed, because synthetic data is often “too clean”, and therefore does not represent well enough the real world. . | Label adaptation: unless the labeling conventions of the synthetic data completely aligns with the ones you use in your test sets, you will need to find a way to adapt the output of your network so that it fits the ground truth of your test set, and not the one of your synthetic data. We have found that the easiest and most effective way is to pretrain your network with synthetic data, and then fine-tune it on real data. . | . Step #5: Scale it until you make it! . Once you have your data-centric process installed, you will need to repeat it as fast as possible, i.e. perform the following steps: . Analyze the failures as described in step 2 → Conclude about the dataset to add . | Generate one or several new datasets based on these insights. . | For each new dataset found: . Retrain the network with this new dataset (+the previous ones to avoid the catastrophic forgetting issue) . | If this new dataset improved the performances in your unit tests: . Keep it for future iterations. . | Go back to step 1 with your freshly trained network . | | If it did not: . Drop this new dataset . | return to step 1 with the previous network . | | | Important note: For the sake of simplicity, I’ve mostly talked about test sets for now, but needless to say that as soon as you start iterating over this test set, and start to make decisions based on your test sets’ results, your final algorithm has a strong risk of overfitting. This is why all this closed loop optimization process should happen with the validation set, and not really the test set per se. . Long-term: AI-powered data-centric AI . At the end of the day, the whole concept of data-centric AI is to engineer the data you need to optimize the performances of your machine learning model in real-life. However, as we saw, figuring out the optimal data you need to solve a task, and how to adapt your training procedure to this data, are very hard algorithmic tasks. Therefore, I believe that the future of data-centric AI is to build a complete closed-loop system (some kind of next-gen AutoML) that not only optimizes your model’s hyperparameters, but also the data it gets fed with. . If you think about this for a second, this idea is actually very comparable to how a child learns. Of course, a child’s brain learns very quickly and efficiently from any data it comes across. However, the data he or she receives is not random at all: it is carefully selected by people who care for the child more than anyone else in the world, and are also (hopefully) themselves very smart: his parents. . However, in order to get there, a lot of very interesting problems need to be addressed: . On the AI side: as we’ve seen, discovering the failure modes of your network, how to adapt your training procedure to new data, and how to optimally generate data based on those failure modes are still research questions, and I personally haven’t found a lot of papers dealing with it. . | On the MLOps side: no matter how smart your algorithms are, you will always need a lot of trials and errors to figure out the best training data and hyper parameters. This means that you will require automated systems that can run as many experiments as possible in parallel, while optimizing all of your hardware resources. Some of these experiments will be related to data gathering, some others related to neural network training, and they will need to communicate and be orchestrated in a very smart way. I actually expect the demand in MLOps tools for DCAI to grow very quickly in the coming years. I personally would love to have such tools. . | . As you can see, data-centric AI is one of the most exciting areas to work on, as it is both THE most impactful way to perform AI, and it is actually itself full of fascinating AI challenges! If working on those challenges gets you excited, you should definitely talk to us! .",
            "url": "https://sebderhy.github.io/impactfulai/2022/03/07/Applied-AI-in-a-Data-centric-World.html",
            "relUrl": "/2022/03/07/Applied-AI-in-a-Data-centric-World.html",
            "date": " • Mar 7, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Risk Management",
            "content": ". One of the most important principles of the Lean Startup methodology is “fail fast”. No one wants to fail, but failing rapidly makes it less painful, and more importantly, leaves more time and resources to try something new until success. . Since AI projects share this characteristic that most of them will fail, we need to carefully design our AI development methodology in order to remove the biggest failure risks as soon as possible: in other words, we need to quickly know when things are not going to work, and change direction when it’s the case. . But of course, this is not as easy as it sounds. Exactly like in startups, the most impactful AI projects seem impossible at the beginning, and only work as a result of perseverance and (sometimes absurd and obsessive) faith. A good example of this is Tesla’s controversial choice to remove LiDAR and HD Maps from their autonomous car perception pipeline: no one thought it would be possible at the time this decision was made, but the tremendous progress of AI since then shows that they may well succeed in this project. . What is the right balance between persistence and agility in AI? How can I execute an AI project in a way that: . Limits as much as possible the risk of failure? . | Makes me quickly realize when I’m going off-track? . | Ensures that if I DO solve the research risks, the rest will follow, and the project will have the expected impact? . | . In this blog post, I will discuss the 4 most important failure risks that I see in AI projects (problem fit, data fit, integration, and research), and will try to give concrete tips and methodology on how to mitigate them. . Risk #1: Problem Fit . . As explained in my previous post, one of the most important risks of failure of an AI project is to build a “Solution In Search of a Problem”. It can happen for many reasons: we may have been over-excited by a new paper or AI technique, and somehow managed to convince ourselves that it would solve our problem, or we may just have an incomplete understanding of the problem because we didn’t talk with the right people at the right time. . In my opinion, the easiest way to reduce this risk is to sit with the people who really feel this problem, and really understand the need. Product managers are in general the right people to have these conversations with, but if you can also access customers and salespeople, this is even better! Through these conversations, you will: . Validate the need for this project . | Design the ideal solution (detached from any technical constraints at first) . | Decide on the reasonable trade-offs that could be made to reduce the project’s risk while maintaining a reasonable value (in other words, the equivalent of a Minimum Viable Product) . | . Ideally, the output of these discussions should be a document with project goals, inputs, expected outputs, and constraints. . On a side note, all these discussions obviously require work and time from the people you’ll discuss with, so you may argue that they don’t have the time to “help you” with your project at such an early stage. I believe it’s actually the opposite: if the project solves a real pain point, they are actually also helping themselves. This time investment actually reduces the AI/problem fit risk much more than the output document itself, which will probably change 30 times anyway. Those people are in general the ones whose interests are the most easily aligned with the company’s core metrics, so if they think they are wasting their time, this is a huge red flag that this project may not be a good use of your time! Actually, even if they do accept investing time, pay attention to their body language, and signs of impatience, it may mean that you are wasting your time too… . Risk #2: Data Fit . We hear all the time that “AI is all about data”, but people often tend to focus on data quantity rather than quality (I’ll explain what I mean by quality). Of course, you cannot do much without a decent amount of data, but it’s usually less than people think (I’ve found that in many cases, ~1000 data points are enough to start getting results). On the other hand, data “accuracy” (how close is your data from the data you will encounter in production) and “variance” (how much variability is there in the data) are critical to success in AI projects. More specifically, you will need to have data (for training AND testing) that reflects as closely as possible the cases you want to handle in production (including corner cases), but which also contains enough variance so that the AI model can learn to extract the patterns that really matter in your data. . However, getting the perfect “data fit” for your problem in one iteration is an almost impossible task. This is why many AI teams are now adopting a new methodology called “Data-Centric AI”, where they basically try to remove as much as possible the friction to acquiring new data (using for example synthetic data which companies like Datagen can provide). While the term has been popularized by Andrew Ng, most advanced companies in AI have been practicing this methodology for years now. My favourite implementation of this methodology is Tesla’s “Data Engine”, which is shown below. Actually, this topic is such a game-changer in the world of AI that I’ll probably dedicate a future blog post to it. . . Source: slide from Andrej Karpathy’s (Tesla’s Director of AI) presentation at Tesla 2019 Autonomy Day . Risk #3: Integration with Other Teams . . Plant a flag on the roadmap . For a long time, I’ve thought that AI projects should live disconnected from product roadmaps, because of their inherent risk. Indeed, since we never know when (or if at all) an AI project is going to succeed, how on earth could we put its output on a roadmap that will afterwards be presented to customers, the company’s board, or maybe even announced publicly?! My conviction was that no one should put an AI-powered feature or product on a roadmap until the project has removed all of the main execution risks. . The problem with that approach is that once the algorithmic risks (i.e. the AI part) are lifted, you still have a very long way to go before seeing the AI in production and delivering actual value: . Other teams don’t know much about your project, so they will suddenly need to deploy a lot of energy to both understand the problem, the solution that you built, and what you need exactly from them. . | You will also need these teams to change their plan, since deploying this feature was not initially on their roadmap. For example, you may need software development to deploy it, write tests, a QA team to check that it actually does what it should, etc… But they also had plans before you came, and no one likes to change their plans… . | The product team has probably committed to other features while you were developing this AI project, so they also won’t be very happy to delay their committed features to push yours. . | Most importantly, since your project did not appear on the roadmap, other teams may engage in structural changes while you develop, and suddenly make the deployment of your AI much harder. Worse, another team may have developed a work-around in the meantime, that may be much less effective than your solution, but will still be very difficult for you to replace. . | . It took me a long time to understand it, but I am now quite convinced that even AI teams should commit to “product” outputs. Yes, there is a risk of not delivering on your promises. Yes, that risk is high. But 1/ Every one who knows a bit about AI is aware of that risk, and 2/ putting an AI achievement on a roadmap before you know it’s feasible (the famous “fake it till’ you make it”) has a lot of advantages that I believe outweighs the risk of not delivering on your promises. Just to name a few: . It will force other teams to get familiar with your project, collaborate with you on it when needed, and invest time to design it properly with you. . | It will also encourage them to plan and allocate resources for the moment your AI solution will be ready. . | Most importantly, it will put a positive pressure on you to build something end-to-end. Maybe it won’t work perfectly, and maybe you won’t be super proud of it at the beginning. But it will hopefully already provide some value to the company, and have the immense advantage of being fully integrated. This will make your life much easier later on when you’ll need to justify 2 additional months of work for the next version (the one with the real fancy AI in it) of this feature. We AI applied scientists don’t work well under pressure, but I believe that the lack of commitment, deadlines and expected deliverables sometimes also hinders us, and it’s time to change that. To continue my beloved parallel with startups, I have often heard that the best companies sell their product before it exists. Why should it be different with AI projects? . | . . Plan interfaces with other teams . One of the biggest points of friction when pushing an AI model to production is integration with other parts of the system. Suddenly you need to have discussions with other teams about architecture: what is the exact format of your inputs/outputs? How will your algorithm be called? By whom? What parts run locally or on the cloud? etc… If you are lucky, these discussions will only generate some “adaptation” work of your algorithm in order to make it look exactly as it should. But if you’re less lucky, these questions may raise significant flaws in your system, and may require you to redesign it entirely. For example, you may assume that you will easily get a certain control or output from another component of the system, and realize later on that this level of output is actually something the other component cannot easily provide. A more obvious example is when your AI requires too much computational resources, and does not match the hardware constraints that you have in production. . Again, I believe the best way to reduce the integration risks is to have discussions with other teams at the beginning of your project on how this solution could be realistically integrated in the final product, which team should be responsible for what, etc… These discussions will also help the other teams involved in this project plan more precisely the resources they’ll need once your project gets more mature. . Risk #4: Research . The research risk is critical, because only you can reduce it. The research phase consists in breaking your problems into subproblems, and building algorithms to solve each of them. Unfortunately, both of those tasks could be very hard, and this is where you’ll get to express all your AI and research talent. Exciting and scary at the same time right? Even though there is unfortunately no magic formula here, below are a few tips from my experience to help alleviate the risks involved with a problem or subproblem. . Connect the dots backwards . . If your AI pipeline is split into several parts, you should start by solving the LAST part, and then progress backwards in the pipeline to solve each block. This may seem counterintuitive at first sight, but there are two simple reasons behind this principle: . You will be able to check early on that your outputs are acceptable in terms of format, quality, etc… . | You will naturally generate accurate specs for the previous blocks in your pipeline. . | For example, let’s say you want to build an AI that recognizes a very small object in images. You may decide to first apply a super-resolution algorithm to your input picture, in order to recognize the object more easily afterwards. But how big should the picture be? What is the required quality for this algorithm? These are questions you can only answer by first developing the recognition block first. . Look at things from a different angle . . Sometimes, we can solve a problem by looking at it from a different and original perspective, typically by mathematically modeling things differently. For example, we can represent a surface quite naturally as a set of connected points in 3D (polygon mesh), but we could also represent it as the 0 level set of a very smooth function of the 3D space f(x,y,z) = distance (potentially signed) of the point (x,y,z) to this surface. This representation was one of the core ideas behind the paper Kinect Fusion, one of the most impactful papers in the world of 3D scanning and localization. This representation inspired the more recent breakthrough paper NeRF (although their representation is a bit different), which is able to render very realistic new views of a scene from several input views. . Map research risks to release versions . Once you have an initial assessment of the research risks involved in your project, try to sit again with the product team, and map them into a roadmap of feature releases. Ideally, you’d like to get to something like this: . v1 (Minimum Viable) v2 (creates significant value) v3 (Super-great amazing holy grail release) . AI/Market Fit | Required | Required | Required | . Integration | Required | Required | Required | . Feature #1 (Research Risk #1) | Required | Required | Required | . Feature #2 (Research Risk #2) | Not required | Required | Required | . Feature #3 (Research Risk #3) | Not required | Not required | Required | . Feature #4 (Research Risk #4) | Not required | Not required | Required | . Make sure you do not have more than 1 strong research risk before you get to a first product release. Indeed, if you have 10% chances to solve a risky problem in general, then you only have 1% chance to solve two risky AI problems in a row, so it’s most likely an unreasonable investment of your time. . Conclusion: should I pivot or persist? . Throughout this blog post, we’ve seen several ways to limit the risks of failure in AI, or at least make sure it happens as fast as possible when it’s inevitable. In particular, we’ve seen that many risks are not just technical but also business-related and organizational. However, the hardest question still remains: how do I know when to keep on trying, and when to change course in an AI project? . Here again, the same question holds for startups, but this time, we have a big advantage in AI over startups: our field is full of mathematics and modelisation. Now is the time to use them! Try to model mathematically your problems and subproblems, convince yourself theoretically or experimentally that something is feasible, or on the contrary try to find counterexamples when you feel your “proof” gets stuck. Simplify your problem into smaller ones that look solvable, and check that you can actually solve it. Try to break the problem into smaller pieces until you find the core resisting piece(s). Finally analyze (maybe even prove) WHY this piece is resisting, and what was the assumption you made there that turned out wrong. This “debug” process may help you make the right decision, but it may also actually help you find the key that you were missing to make your AI project work! .",
            "url": "https://sebderhy.github.io/impactfulai/2021/12/06/Risk-Management.html",
            "relUrl": "/2021/12/06/Risk-Management.html",
            "date": " • Dec 6, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Fall In Love With The Problem, Not The Solution",
            "content": ". (A big thanks to Elodie Tordjman from Smart-content for her great advice on this blog post) . Have you ever been given a task/project starting with: “Wouldn’t it be great if we had capability X? It would be a game-changer in so many aspects for our company!”. While this sounds like a very positive and energizing task, I’ve become more and more skeptical over the years of such tasks, because they lead you to build a “Solution In Search of a Problem”, a term popularized by Y Combinator to describe one of the most common startup pitfalls: . “The third mistake is to start with a solution instead of a problem. […] We see this so commonly at YC that we have a term for it. It’s called a “Solution In Search of a Problem,” or a SISP. And these are usually not great, because usually, you never actually find the problem. You’re much better off starting with a problem, and then looking for solutions.” . I’ll argue that what is true for a startup is actually also very true for AI algorithms. Actually, even if a task is properly defined as a problem, we often tend to jump on building a solution, rather than investigating the pain point, simply because it is our comfort zone: after all, we’ve all been trained and hired for our “problem-solving” skills right? Who wants to have a “problem-digger”? . This post will be divided into 2 parts: . How this SISP trend materializes in the AI world (both academic and industrial). . | 7 practical tips to make sure your AI solution stays focused on solving a real and meaningful problem. . | Examples from the real world - it happens more than you’d think! . An example from the industry . . In 2011, Microsoft released a consumer gaming accessory called Kinect that enabled people to play by moving freely in front of their camera. In addition to being a huge commercial success, it was also a very hard technological achievement. The Kinect team not only managed to build a brilliant gesture recognition algorithm, but also made a huge hardware achievement: in order to provide a truly seamless experience, Microsoft realized that they would need a special type of camera, called “depth camera” (a camera able to estimate the 3D location of each pixel). They found a way to miniaturize them, make them much cheaper, and manufacture them at scale, partnering with an Israeli startup called Primesense, which was later acquired by Apple in a surprising turn of events. Kinect is in my opinion the perfect example of a problem-driven technological breakthrough, and this is in my opinion why it was so successful. . However, as is very often the case, this technological achievement somehow gave birth to many solution-driven technological failures. Suddenly, other big companies realized that these cameras were able to achieve what regular cameras couldn’t and that these cameras would “open doors to many amazing new applications for smartphone and tablet cameras”. Google opened a project called Tango, Intel a project called RealSense, and Apple acquired Primesense. . Fast-forward 10 years later, what happened to these projects? Well, the results are unfortunately not brilliant. Google managed to manufacture a few devices, but didn’t achieve any commercial success with Tango, and shut down the program. Intel manufactured a few RealSense products, built a lot of software, but finally didn’t reach any meaningful commercial success and recently announced that they will be shutting down their RealSense department. Apple did achieve commercial success with iPhones and iPad equipped with depth cameras, but it’s hard to tell whether they ever found a “killer-app” for depth cameras. My guess is that currently, the most important use-case of the iPhone’s depth camera is FaceID, which allows you to unlock your smartphone with your face. And yes, FaceID is cool, but will it be remembered as a game-changing technology? Is it really much more robust than fingerprint-based or regular-camera-based identification? . An example from the academy . In 2017, some researchers from Google introduced the Transformers architecture, and it was a huge breakthrough in the world of NLP/NLU (Natural Language Processing/Understanding) because it enabled a dramatic performance boost on almost every language-related AI task. But exactly like in my previous examples, after the seminal paper introducing Transformers, many people tried to apply Transformers to other domains, and in particular computer vision, and even managed to show some slight improvements on ImageNet classification. However, in computer vision, we already have an architecture that works really well: Convolutional Neural Networks (CNN). Actually, an amazing recent paper called “ResNet Strikes back” proved that the most trivial CNN architecture can reach the same level of performance as Transformer-based architectures if they are trained correctly. Of course, this doesn’t mean that transformer will never be useful for images, but why not rather spend time to leverage transformers in much harder AI problems where CNNs do not work well, such as audio processing, geometric deep learning, few-shots classification, 3D mesh processing, etc…? . That being said, I have to admit that I’m being a bit unfair with the AI academic world. Maybe academic research SHOULD be solution-driven and not problem-driven. Some problems in the world are so hard to fix that the only way to solve them is by trying lots of things and “connecting the dots backward” (as Steve Jobs famously said). The world obviously needs some solution-driven research to accomplish great things, but people engaging in this path should be aware that their impact will at best be in the long run, at worst be null. . 7 tips to make sure you are building a problem-driven AI . Get business insights about the problem . . Whenever you receive a task, it is very likely that the person giving you the task (e.g. an Algorithm Team Lead) is not the real source of the task. More likely, they heard from other people in the company about a painful problem that needs to be addressed and thought it could be a good fit for your skills. . Therefore, your first goal should be to become the most knowledgeable person about this problem: go and talk to the people that really FEEL the pain. Very often, this should involve talking to customers, but if it’s difficult for any reason, try to at least ask as much about it to your Product Manager, or customer-facing people. In particular, I recommend this user interview framework from Startup School when trying to understand a problem: . What’s the hardest part about the problem? . | Tell me about the last time you found this problem? . | Why was that hard? . | What, if anything, have you done to try to solve this problem? . | What are the issues with the solutions you’ve tried? . | . Another famous and in my opinion very efficient framework to understand the root cause of a problem is to ask the 5 whys. This will really help you get to the core reasons behind your problem, and I found that it can be particularly helpful later on when you’ll need to find creative solutions to solve the problem. . Check that the problem is correctly prioritized . Once you understand the problem, try to get a very rough sense of its worth. How often does the company lose a deal because of this? How much engineering time would be saved if it were solved? How important is this problem compared to others that you could work on? . This is actually a very tricky part. First, because prioritization requires having the big picture, and this is something you don’t always have. In addition, as an AI scientist, you usually don’t have the luxury to decide whether a task should or should not be prioritized. However, you should always have the ability (it’s even a duty!) to discuss with the person that gave you this task, and try to change their mind if you don’t agree with them. Making correct prioritization is probably one of the toughest job leaders have, so help them be better at their job: they will appreciate it, and you will avoid working in vain. I’ve seen too many AI scientists making amazing solutions that never made it to production because they turned out not to be so important in the end, and hence the deployment task never got prioritized. . Prove that this problem really requires you . . Especially if you’ve done the 5 whys exercise, you should now see the deep root causes of the problem you are trying to solve. At that point, you may realize that the main problem might be solved by something very different from what you’ve been originally told. It could potentially be anything: a different marketing strategy, a better internal process, or simply the integration of some external software. . The most frequent example is when you find a third-party company that already offers a product that solves your problem. Of course, it will cost money, but unless the third-party company has a VERY bad pricing strategy, this solution should be a better deal than building it in-house from scratch. So before you start writing a single line of code, make sure that you’ve done serious market research, and that a commercial product suiting your needs does not already exist. . Build a good evaluation pipeline . If you’ve done your research correctly on the problem understanding part, you should already have a good idea of what an ideal solution looks like. You may even have a few brilliant ideas on how to tackle the problem. However, it is very important that you refrain from coding your brilliant solution, otherwise, you’ll have no idea how good it is, and unfortunately, no one will be convinced that it works just because it comes from your brilliant mind, or because it is based on the latest CVPR award-winning paper. . At this stage, you should design a robust and informative way to evaluate any solution to the problem you’re solving: . Quantitative performance tests: metrics that really characterize how good your solution is. In general, one needs to define several metrics that are anti-correlated, such that it is very hard to increase one without hurting the other. . | Qualitative performance tests: even though it’s important to define the algorithm’s quality through clear metrics, these metrics need in general to be complemented with human-based judgment. For example, if you are building an AI that generates images, and want to evaluate how good-looking these images are, you can use metrics such as FID or PPL, but at some point, someone will have to take a look at them (spoiler: most of the time, YOU will have to!!). . | Divide the performance tests above into test sets that . Represents as faithfully as possible the data you’ll have in production . | Help you find your solution’s strengths and weaknesses before being deployed . | . | . Needless to say that the process to test a given solution and visualize its results should be as straightforward and convenient as possible (one push button, ideally). It will save you a lot of precious time later when you’ll need to iterate. On this topic, I highly recommend tools such as Tensorboard and Weights &amp; Biases for tracking machine learning results, or QA-Board for more generic algo projects (QA-Board has in particular an amazing image comparison tech). You may also want to use tools such as Kubeflow that enable you to manage and run many machine learning experiments in parallel. I’ll probably make a separate blog post on MLOps, so that we can dive more into those tools and the pain points they solve. . Interestingly, this idea that one should write problem-defining tests before coding the solution that will pass those tests is very famous and widely adopted in the world of software development. It’s called test-driven development, and Robert Martin (Uncle Bob) has a wonderful presentation on this topic. For some reason, its adoption in the world of AI is still relatively low. . Think big, start small . . One of the previous companies I worked for (Via Transportation) had this amazing sentence from the marketing genius Seth Godin written on their walls to motivate R &amp;D teams. They wanted to encourage employees to aim high in their projects, but also be able to quickly make a step that would move the company forward. . Now that you’ve set up a great testing environment, your next step is to build a baseline, i.e. the SIMPLEST algorithm you can write in your test environment that will get you ANY performance score (yes, this score does need to be great). Don’t try to be too sophisticated there, the goal is to have something to compare to, and to get some convincing evidence that the complicated algorithm you will build next really has to be complicated. . Iterate fast . . Once you have a baseline and a good evaluation environment, the algo fun can finally begin! This is the place where your research talent comes most into play: read papers, get interesting insights about the data and your problem, implement things, etc… However, keep in mind that it’s not just about being smart, but also being able to iterate fast. Here again, the resemblance with startups is stunning: because of the very uncertain environment of algorithmic research, it is often VERY worth spending time on the iteration speed process rather than figuring out the next thing you want to try. Typically, it is in general a bad idea to train a network for 7 days if you have no idea whether it’s going to work in the end. . My former boss at Samsung Nathan Levy gave me a great rule of thumb to decide when to invest in iteration speed vs implementing your next idea: whenever you find yourself waiting for something to finish (e.g. a network is training), you may need to focus this waiting time on accelerating your development cycle (e.g. improve the runtime of a bottleneck function), rather than working on your next algorithmic move. . Move to prod as fast as possible . . Ok, so you’ve finally got your first good-enough algorithm, but at this stage, chances are you’re still not satisfied… There are so many things you haven’t optimized yet! So many other ideas you’d like to experiment! But this is also where you need the discipline to focus on your problem (the one you fell in love with, remember :)?). Even though your algorithm seems pretty basic and very improvable, you should try to push it to production as soon as possible, since it is probably already better than what exists today. . The reasons for this are: . The process to move things to prod is slow and painful. You’ll probably have plenty of time to improve your algorithm during the deployment process, and in most cases, you will be able to update your model/code without problems. . | The process of deploying will likely teach you a lot about the constraints your solution needs to fit in, and you might suddenly realize that some key performance aspect of your algorithm was not properly taken into account by your evaluation metrics/pipeline: in other words, you’re back to step 4 ;-). . | Once you’ve made the full cycle of moving to prod, you should be able to iterate and deploy improvements on a regular basis. This will make your work much more visible, and if you care about the problem you’re solving, you’ll feel great that your work actually makes someone else’s life better. . | Conclusion - Wait, is this really my job?!! . You may have noticed that most of the tips in this article don’t require reading lots of papers, implementing super-fancy-math-based AI algorithms, etc… So I think that it’s important for me to clarify a few things. . I didn’t write a lot about step #6 (will probably explain more about fast iteration in a future post), but it is probably the hardest, longest, and most uncertain part of an AI development process, so in practice, you can expect to spend a significant time on researching a good solution for your problem. But don’t forget the other steps! . | In some organizations, other people (e.g. product managers) will perform steps 1 to 3, so that you can focus on others that require more coding. Therefore, if you only want to code, some organizations may help you do that. . | That being said, let’s face the ugly truth: real-life impactful AI is not academic research, and it’s not even a Kaggle competition. If you are intrinsically motivated by the value you’re creating for the world, and the cause you are helping move forward, you’ll eventually thrive, be recognized for your work, and find huge satisfaction in seeing the impact of your work. On the other hand, if you only want to do the fun and interesting parts of AI, read and write papers or learn new technical skills, you are probably better off doing academic research, which by the way is also a great path that I highly respect. It’s just a different one! . |",
            "url": "https://sebderhy.github.io/impactfulai/2021/11/01/Fall-in-love-with-the-problem,-not-the-solution.html",
            "relUrl": "/2021/11/01/Fall-in-love-with-the-problem,-not-the-solution.html",
            "date": " • Nov 1, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Why This Blog",
            "content": "Over the last 10 years, I’ve realized that applied research (and more specifically AI) is a double-edged sword. On the one hand, it can help you solve incredibly complex problems, and create an insane Return-On-Investment for companies. But on the other hand, according to a Venturebeat article (based on a Gartner report), only 20% of AI projects ever get deployed to production, and among those, only 40% are profitable! Unfortunately, these figures do not surprise me, as I’ve seen countless algorithmic and machine learning projects fail in my career. . I created this blog in order to analyze the reasons behind such failures, share experiences (mine and others’), and generate discussions that could eventually help maximize the impact of AI research in the real world. . Learning from the Startup World . In this blog, I will often tackle the questions raised above with an innovative angle: adapt lessons from the startup world to the AI conceptual world. Indeed, although we often hear about how startups can leverage AI to improve their business, no one to my knowledge took the opposite angle and explored how to leverage the widely studied and experienced startup world to improve AI’s impact on the world. . This idea may sound very weird at first sight, but anyone who looks closely at those two systems will be stricken by their similarities: a high risk of failure, an uncertain environment, the need for agility and speed, the urge to convince many stakeholders and align their interests, the necessity to find harmony between tech and business, … . Last but not least, the similarities between those 2 worlds particularly resonate with me, since I have been a VC investor and founder in the past, invested myself in a few startups, and generally spend a lot of time discussing and working with entrepreneurs. . Who is this blog for? . At a high level, this blog is for people that - like me - believe that AI’s impact should be much larger than it is today. More specifically, I think 3 types of people can learn from this blog: . Applied researchers like myself, who love reading papers and solving hard technical problems, but who are more motivated by their actual impact than their salary or implementing the latest cool AI paper (who said GAN ;-)?). . | Entrepreneurs and leaders, technical or not, on questions such as: “what to look for when recruiting?”, “what kind of ML infrastructure will make my teams most effective?”, “When to know when to stop an AI project”, etc… . | Investors or future entrepreneurs, on questions like “what are the problems in the world of AI today”, “who is tackling them and how”, etc… . | . Terminology . There are many close terminologies in the world of AI, and you will notice that I often use terms such as Artificial Intelligence, Data Science, Algorithms, Applied Research in an interchangeable manner… Some people may tell you that they are very different (I personally don’t think so), and there are many debates about differences between those fields. However, the truth is that in our case, the terminology doesn’t make such a difference, because they all relate to the same problem: how to handle risky projects that can have a huge impact, but whose results are quite unpredictable, and can often also very easily fail. Again, you can see here why I believe startups can teach us a lot on those projects. .",
            "url": "https://sebderhy.github.io/impactfulai/2021/10/22/Why-This-Blog.html",
            "relUrl": "/2021/10/22/Why-This-Blog.html",
            "date": " • Oct 22, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". My name is Sebastien Derhy, I am originally from Paris, but live in Tel-Aviv since July 2014. I’ve always been passionate about the intersection between technology (in particular deep learning and computer vision), research, and business. This is one of the reasons creating this blog was so important to me. . I am currently Lead Applied Scientist at Datagen, a startup helping computer vision scientists work seamlessly in a data-centric R&amp;D cycle using synthetic data. . My past experiences include: . Navigation Intelligence Team Leader at Via Transportation | Algorithm Team Leader at Samsung Israel Research Center | COO &amp; Co-founder of Fitterli, a startup that aimed at leveraging depth cameras in smartphones for online shopping | VC at Elaia Partners, one of the leading European VCs, first investors of several French unicorns: Criteo (NASDAQ: , CRTO), Shift Technology, and Mirakl. | Visiting Researcher at the Technion, where I worked at the GIP lab under the supervision of Ron Kimmel on a Gesture-based Tetris game. | . I hold an MSc in Applied Mathematics from Ecole Polytechnique and an MSc in Entrepreneurship from HEC. .",
          "url": "https://sebderhy.github.io/impactfulai/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sebderhy.github.io/impactfulai/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}